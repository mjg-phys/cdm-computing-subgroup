{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Hxy3riK7hElf"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mjg-phys/cdm-computing-subgroup/blob/main/MLTutorial/NNtutorial.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rf258xT0XIwV"
      },
      "source": [
        "# CDMPP Computing Subgroup Workshop Machine Learning Tutorial: Training a Jet Tagging with **DNN**\n",
        "\n",
        "---\n",
        "In this notebook, we perform a Jet identification task using a multiclass classifier based on a\n",
        "Dense Neural Network (DNN), also called multi-layer perceptron (MLP). The problem consists on identifying a given jet as a quark, a gluon, a W, a Z, or a top,\n",
        "based on set of physics-motivated high-level features.\n",
        "\n",
        "For details on the physics problem, see https://arxiv.org/pdf/1804.06913.pdf\n",
        "\n",
        "For details on the dataset, see Notebook1\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4OMAZgtyXIwY"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import h5py\n",
        "import glob\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2lbB-J3hXIwb"
      },
      "source": [
        "# Preparation of the training and validation samples\n",
        "\n",
        "---\n",
        "In order to import the dataset, we now\n",
        "- clone the dataset repository (to import the data in Colab)\n",
        "- load the h5 files in the data/ repository\n",
        "- extract the data we need: a target and jetImage\n",
        "\n",
        "To type shell commands, we start the command line with !\n",
        "\n",
        "**nb, if you are running locally and you have already downloaded the datasets you can skip the cell below and, if needed, change the paths later to point to the folder with your previous download of the datasets.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jWjxFaRPXIwb",
        "outputId": "a0ee3bc8-dc1d-4577-c565-aaba5995a113"
      },
      "outputs": [],
      "source": [
        "! curl https://cernbox.cern.ch/s/zZDKjltAcJW0RB7/download -o Data-MLtutorial.tar.gz\n",
        "! tar -xvzf Data-MLtutorial.tar.gz\n",
        "! ls Data-MLtutorial/JetDataset/\n",
        "! rm Data-MLtutorial.tar.gz"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cCGhrKdwXIwc",
        "outputId": "94f64d7e-0c11-49b3-e0c0-ebae19a29cc9"
      },
      "outputs": [],
      "source": [
        "target = np.array([])\n",
        "features = np.array([])\n",
        "# we cannot load all data on Colab. So we just take a few files\n",
        "datafiles = ['Data-MLtutorial/JetDataset/jetImage_7_100p_30000_40000.h5',\n",
        "             'Data-MLtutorial/JetDataset/jetImage_7_100p_60000_70000.h5',\n",
        "             'Data-MLtutorial/JetDataset/jetImage_7_100p_50000_60000.h5',\n",
        "             'Data-MLtutorial/JetDataset/jetImage_7_100p_10000_20000.h5',\n",
        "             'Data-MLtutorial/JetDataset/jetImage_7_100p_0_10000.h5']\n",
        "# if you are running locally, you can use the full dataset doing\n",
        "# for fileIN in glob.glob(\"Data-MLtutorial/JetDataset/*h5\"):\n",
        "\n",
        "for fileIN in datafiles:\n",
        "    print(\"Appending %s\" %fileIN)\n",
        "    f = h5py.File(fileIN)\n",
        "    myFeatures = np.array(f.get(\"jets\")[:,[12, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 52]])\n",
        "    mytarget = np.array(f.get('jets')[0:,-6:-1])\n",
        "    features = np.concatenate([features, myFeatures], axis=0) if features.size else myFeatures\n",
        "    target = np.concatenate([target, mytarget], axis=0) if target.size else mytarget\n",
        "    f.close()\n",
        "print(target.shape, features.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AWR8hDKu-xrH"
      },
      "source": [
        "The dataset consists of 50000 jets, each represented by 16 features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Mhnz4TuiOvQO"
      },
      "source": [
        "For those who are curious, our jet-level variables we are using are:\n",
        "~~~\n",
        "12 - b'j_zlogz'\n",
        "34 - b'j_c1_b0_mmdt'\n",
        "35 - b'j_c1_b1_mmdt'\n",
        "36 - b'j_c1_b2_mmdt'\n",
        "37 - b'j_c2_b1_mmdt'\n",
        "38 - b'j_c2_b2_mmdt'\n",
        "39 - b'j_d2_b1_mmdt'\n",
        "40 - b'j_d2_b2_mmdt'\n",
        "41 - b'j_d2_a1_b1_mmdt'\n",
        "42 - b'j_d2_a1_b2_mmdt'\n",
        "43 - b'j_m2_b1_mmdt'\n",
        "44 - b'j_m2_b2_mmdt'\n",
        "45 - b'j_n2_b1_mmdt'\n",
        "46 - b'j_n2_b2_mmdt'\n",
        "48 - b'j_mass_mmdt'\n",
        "52 - b'j_multiplicity'\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6oq-eQGIUm33"
      },
      "source": [
        "For our first NN, we only wish to do top tagging between a top jet and a quark jet. So we will filter out all the other jets for now.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BImXkfAwsd4v",
        "outputId": "fa07069d-e455-4ccc-daff-81af8099211b"
      },
      "outputs": [],
      "source": [
        "# Step 1: Identify indices where the target meets the criteria (1 in the first or fifth slot)\n",
        "mask = (target[:, 0] == 1) | (target[:, 4] == 1) # [:,1] is gluons (u,d,s), [:,4] is top\n",
        "\n",
        "# Step 2: Filter features and target arrays based on the mask\n",
        "filtered_features = features[mask]\n",
        "filtered_target = target[mask]\n",
        "\n",
        "# Step 3: Create the new target 'isTop' based on the criteria\n",
        "# If 1 in the first slot -> isTop = 0, if 1 in the fifth slot -> isTop = 1\n",
        "filtered_target = np.where(filtered_target[:, 0] == 1, 0, 1)\n",
        "\n",
        "print(filtered_target)\n",
        "print(sum(filtered_target))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6a333RYPXIwe"
      },
      "source": [
        "We now shuffle the data, splitting them into a training and a validation dataset with 2:1 ratio"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZBqFs1eBXIwf",
        "outputId": "613aec80-0760-41ca-a0c0-e95645816d64"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_val, y_train, y_val = train_test_split(filtered_features, filtered_target, test_size=0.33)\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkNz5UAhXIwg"
      },
      "source": [
        "# DNN model building"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ucM4HKrE5zi0"
      },
      "source": [
        "There are two main Pythonic packages to perform ML: **tensorflow/keras** & **PyTorch**.\n",
        "\n",
        "In this tutorial we will focus on using TensorFlow/keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tTSDOiEHXIwh"
      },
      "outputs": [],
      "source": [
        "# keras imports\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.layers import Dense, Input, Dropout, Flatten, Activation\n",
        "from tensorflow.keras.utils import plot_model\n",
        "from tensorflow.keras import backend as K\n",
        "from tensorflow.keras import metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rAl0DZTxXIwi"
      },
      "outputs": [],
      "source": [
        "input_shape = X_train.shape[1]\n",
        "dropoutRate = 0.25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OUcrn2Fn5cmU"
      },
      "source": [
        "# Challenge 0: Lecture Example (5 mins)\n",
        "\n",
        "We will build our simple network from the lecture"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2l492G8BXIwj"
      },
      "outputs": [],
      "source": [
        "####\n",
        "inputArray = Input(shape=(input_shape,))\n",
        "#\n",
        "x = Dense(2, activation='relu')(inputArray)\n",
        "x = Dropout(dropoutRate)(x)\n",
        "#\n",
        "output = Dense(1, activation='sigmoid')(x)\n",
        "\n",
        "\n",
        "# output = Dense(1, activation='sigmoid')(x)\n",
        "####\n",
        "model = Model(inputs=inputArray, outputs=output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IboLANZkTNTJ"
      },
      "source": [
        "The Dropout layer in Keras randomly sets input units to 0 with a frequency rate at each step during training, helping prevent overfitting."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6bil_I55Y3d"
      },
      "source": [
        "We can then compile our model. We are using the \"adam\" optimiser to perform our gradient descent, see https://arxiv.org/abs/1412.6980"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xu8rRUkhXIwj",
        "outputId": "b8421c71-6545-49f5-e184-eaf1e6cac776"
      },
      "outputs": [],
      "source": [
        "model.compile(loss='BCE', optimizer='adam')\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2HfKWoOtXIwk"
      },
      "source": [
        "We now train the model with these settings:\n",
        "\n",
        "- the **batch size** is a hyperparameter of gradient descent that controls the number of training samples to work through before the model internal parameters are updated\n",
        "    - batch size = 1 results in fast computation but noisy training that is slow to converge\n",
        "    - batch size = dataset size results in slow computation but faster convergence)\n",
        "\n",
        "- the **number of epochs** controls the number of complete passes through the full training dataset -- at each epoch gradients are computed for each of the mini batches and model internal parameters are updated.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzO-lyLEXIwk",
        "outputId": "da949561-a67e-4f43-d4e3-6fd99fbd7932"
      },
      "outputs": [],
      "source": [
        "batch_size = 128\n",
        "n_epochs = 25\n",
        "\n",
        "# train\n",
        "history = model.fit(X_train, y_train, epochs=n_epochs, batch_size=batch_size, verbose = 2,\n",
        "                validation_data=(X_val, y_val))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYHW6KQGSPxw"
      },
      "source": [
        "Note: Running \"fit\" on a model that has already been fitted, with continue to train the same model. But it will not save its previous history... be careful!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "044bCLqVXIwl",
        "outputId": "9163b278-fe6a-4254-dc04-96b3da42aae5"
      },
      "outputs": [],
      "source": [
        "# plot training history\n",
        "plt.plot(history.history['loss'])\n",
        "plt.plot(history.history['val_loss'])\n",
        "# plt.yscale('log')\n",
        "plt.title('Training History')\n",
        "plt.ylabel('loss')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['training', 'validation'], loc='upper right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oESSmNLxXIwm"
      },
      "source": [
        "## Evaluating\n",
        "\n",
        "To evaluate we will build a ROC Curve, and calculate the auc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 472
        },
        "id": "gjKT7EjUXIwn",
        "outputId": "c4c77d00-8d34-4b3b-952c-1961e3042e9f"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from sklearn.metrics import roc_curve, auc\n",
        "predict_val = model.predict(X_val)\n",
        "df = pd.DataFrame()\n",
        "fpr = {}\n",
        "tpr = {}\n",
        "auc1 = {}\n",
        "plt.figure()\n",
        "label = 'top'\n",
        "\n",
        "df[label] = y_val\n",
        "df[label + '_pred'] = predict_val\n",
        "\n",
        "fpr[label], tpr[label], threshold = roc_curve(df[label],df[label+'_pred'])\n",
        "\n",
        "auc1[label] = auc(fpr[label], tpr[label])\n",
        "\n",
        "plt.plot(fpr[label],tpr[label],label='%s tagger, auc = %.1f%%'%(label,auc1[label]*100.))\n",
        "# plt.semilogy()\n",
        "plt.xlabel(\"False Positive Rate\")\n",
        "plt.ylabel(\"Truth Positive Rate\")\n",
        "plt.ylim(0.000001,1)\n",
        "plt.grid(True)\n",
        "plt.legend(loc='lower right')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-8q34gMmWcca"
      },
      "source": [
        "Fun little note: Why would you sometimes see that ``val_loss`` converges to `` 0.6930`` and predict with ``auc ~ 50%``?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRxygEK1dW2g"
      },
      "source": [
        "# Optional Challenge 0: Changing Learning Rate\n",
        "\n",
        "By default, the learning rate is 1e-3. However, we might want to change it. One way of doing this is to import the Adam optimiser, and set the learning rate directly. i.e.\n",
        "\n",
        "```\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "optimizer = Adam(learning_rate=0.0001)  # Example learning rate\n",
        "\n",
        "# Compile the model with the optimizer\n",
        "model.compile(optimizer=optimizer, loss='BCE')\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Wko2xsJM5HBV"
      },
      "source": [
        "# Challenge 1: Improving our Neural Network (20 mins)\n",
        "\n",
        "We have a relatively simple model, being a single layer with only 2 nodes. Lets see how we can make it more complex.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MjTYGdc0beFQ"
      },
      "source": [
        "## Model Building\n",
        "\n",
        "*Hint*: Copy the model from above and then\n",
        "1. Add 3 more hidden layers, so there is a total of 4 hidden layers and have the layer sizes of (80,40,20,10).\n",
        "   - You can do this by copying the ``x =`` lines, and replace the ``inputArray``  with ``x``\n",
        "2. Add a \"Dropout\" layer between the hidden layers\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lzbQ-d0RKVmV"
      },
      "outputs": [],
      "source": [
        "### SOLUTION ####\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kR1CfAmbsP6"
      },
      "source": [
        "## Training\n",
        "\n",
        "*Hint*: Copy the code from above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZKeeCztl-AA"
      },
      "outputs": [],
      "source": [
        "### SOLUTION ###\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "beQ8OOi2m-kF"
      },
      "outputs": [],
      "source": [
        "# plot training history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4U7wuqAkcfRG"
      },
      "source": [
        "## Evaluating\n",
        "\n",
        "*Hint*: Copy the code from above"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pi8rxUgPyBmx"
      },
      "outputs": [],
      "source": [
        "### SOLUTION ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t98IuLdd48N6"
      },
      "source": [
        "Hopefully it improved? Maybe it didnt... Play around with hyperparameters such as learning rate, batch size and hidden layer size to see if you can get an improvement...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BidazkvH4yJd"
      },
      "source": [
        "# Optional Challenge: Adding Callbacks\n",
        "\n",
        "At the moment, we have no extra conditions on our training. What if we are running our network for 100 epochs but we get no improvement after the first 10? It would be a waste of compute time (and our time) to continue to train.\n",
        "\n",
        "**callbacks** are algorithms used to optimize the training (full list [here](https://keras.io/api/callbacks/)):\n",
        "    - *EarlyStopping*: stop training when a monitored metric (`monitor`) has stopped improving in the last N epochs (`patience`)\n",
        "    - *ReduceLROnPlateau*: reduce learning rate when a metric (`monitor`) has stopped improving in the last N epochs (`patience`)\n",
        "    - *TerminateOnNaN*: terminates training when a NaN loss is encountered"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JB7KyXia8Bn_"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, TerminateOnNaN"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WYwDK28S8HMn"
      },
      "source": [
        "To use, just add \"callbacks\" to the .fit(...) like\n",
        "\n",
        "~~~\n",
        "history = model.fit(<other stuff>,\n",
        "                callbacks = [\n",
        "                EarlyStopping(monitor='val_loss', patience=10, verbose=1),\n",
        "                ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=2, verbose=1),\n",
        "                TerminateOnNaN()])\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmV9H-0eXB_D"
      },
      "source": [
        "Try adding callbacks to the model... you might need to re-make a new model first!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aB8HZ6z48OSd"
      },
      "outputs": [],
      "source": [
        "### SOLUTION ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XX98OnXjAe8N"
      },
      "source": [
        "# Challenge 2: Make it predict on all of the different categories  (25mins)\n",
        "\n",
        "In our dataset we have access to 5 different types of jet, what if we want to build a classification model to classify between all of these?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-y4NABQSnB4Z"
      },
      "outputs": [],
      "source": [
        "for fileIN in datafiles:\n",
        "    print(\"Appending %s\" %fileIN)\n",
        "    f = h5py.File(fileIN)\n",
        "    myFeatures = np.array(f.get(\"jets\")[:,[12, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 48, 52]])\n",
        "    mytarget = np.array(f.get('jets')[0:,-6:-1])\n",
        "    features = np.concatenate([features, myFeatures], axis=0) if features.size else myFeatures\n",
        "    target = np.concatenate([target, mytarget], axis=0) if target.size else mytarget\n",
        "    f.close()\n",
        "print(target.shape, features.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ro-JoeTszZeG"
      },
      "outputs": [],
      "source": [
        "X_train, X_val, y_train, y_val = train_test_split(features, target, test_size=0.33)\n",
        "print(X_train.shape, X_val.shape, y_train.shape, y_val.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u1UGjPEK8wQb"
      },
      "source": [
        "## One-hot-encoding\n",
        "\n",
        "One hot encoding converts categorical variables into binary vectors representing each category's presence (1) or absence (0).\n",
        "\n",
        "Example:\n",
        "~~~\n",
        "[1,0,0,0,0], # quark\n",
        "[0,1,0,0,0], # W\n",
        "[0,0,1,0,0], # Z\n",
        "[0,0,0,1,0], # gluon\n",
        "[0,0,0,0,1], # top\n",
        "~~~"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q408rUpJ9Ssm"
      },
      "source": [
        "## Model Building...\n",
        "\n",
        "*Hint:* Copy your model from the previous example and change\n",
        "1. The output: 1  -> 5\n",
        "2. The loss function: \"BCE\"  -> \"categorical_crossentropy\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-QC-3jBHzfKb"
      },
      "outputs": [],
      "source": [
        "#### SOLUTION #####\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EoggLr1V9gQZ"
      },
      "source": [
        "## Training:\n",
        "\n",
        "Hint: Copy the code from before, it should work!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kxrEmUvZ9nrm"
      },
      "outputs": [],
      "source": [
        "### SOLUTION ###\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HnwVSgJ1eOrc"
      },
      "outputs": [],
      "source": [
        "# plot training history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IO0ovFfh9ra2"
      },
      "source": [
        "## Evaluating:\n",
        "\n",
        "*Hint*:  Copy the ROC curve code here\n",
        "1. Loop over the labels container\n",
        "2. Isolate the prediction for each particle type\n",
        "3. Calculate the ROC curve/auc for particle\n",
        "4. Plot on the same axis\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6EyxfZk6z6aX"
      },
      "outputs": [],
      "source": [
        "### SOLUTION ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V9lBMpB2fSwk"
      },
      "source": [
        "Once you have done this and if there is extra time, play around with the hyperparameters to try and get better auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A2Z6ZDCI99sy"
      },
      "source": [
        "# Optional Challenge 2: Confusion Matrix\n",
        "\n",
        "A confusion matrix is another way to evaluate the performance of a classification model. It is a matrix which compares the truth value to the predicted value. See [link](https://www.analyticsvidhya.com/blog/2020/04/confusion-matrix-machine-learning/#:~:text=A%20confusion%20matrix%20is%20a%20performance%20evaluation%20tool%20in%20machine,false%20positives%2C%20and%20false%20negatives.)\n",
        "\n",
        "This might make the result more understandable. For example, if we know there is confusion between two particle types (i.e. $W$'s and $Z$'s) we might have more of an insight on where our model breaks down. We can use the functions from:\n",
        "\n",
        "```\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "import seaborn as sns #to plot\n",
        "import tensorflow as tf\n",
        "```\n",
        "\n",
        "One thing you will have to do is convert the one-hot-encoding to a single value. The suggestion is to use ``np.argmax(y_val)`` and ``np.argmax(predict_val)`` , and sub these into  `` tf.math.confusion_matrix()``"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fquti-zY94uB"
      },
      "outputs": [],
      "source": [
        "### SOLUTION ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R2mAOQM2A6PH"
      },
      "source": [
        "# Challenge 3: Regression instead of classification (30mins)\n",
        "\n",
        "It is common to instead use NN's to predict on continous value rather than a simple classification. We will do some regression on the jet mass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U9QcGDyMBUai"
      },
      "source": [
        "## Data preperation:\n",
        "\n",
        "\n",
        "*Hint*: Copy the code for getting the features and targets. Change the target to be the jet mass (48th element of the jet features). To make sure you have the mass, plot the target! Make sure you remove the jet mass from the feature list! It might also be useful to use the ``train_test_split`` function to keep track of the truth particle type by adding another arguement. i.e\n",
        "```\n",
        "X_train, X_val, y_train, y_val, ohe_train, ohe_val = train_test_split(features, target,ohe, test_size=0.33)\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C6K-TMlT1Fx8"
      },
      "outputs": [],
      "source": [
        "### SOLUTION ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWqJaju6BzJx"
      },
      "source": [
        "## Model Building\n",
        "\n",
        "*Hint*: Copy the code model from before.\n",
        "\n",
        "1. Change the output back to 1.\n",
        "2. Change the loss function to be \"MSE\" (or any other regression loss function)\n",
        "3. Change the final layers activation fuction to be \"linear\" instead of sigmoid"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Z39qVjPTCGXd"
      },
      "outputs": [],
      "source": [
        "### SOLUTION ###\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "prXiqZvoCJSR"
      },
      "source": [
        "## Training:\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "z_5KMQZ4T7An"
      },
      "outputs": [],
      "source": [
        "### SOLUTION ###\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NmgOqt0Go0o8"
      },
      "outputs": [],
      "source": [
        "# plot training history\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mCHNWmNHT9aY"
      },
      "source": [
        "## Evaluating:\n",
        "*Hint*: Since we are no longer performing classification, a ROC curve does not make sense to use. We willl be using the response, which mathematically is\n",
        "$$\n",
        "Response = \\frac{M_{prediction}}{M_{truth}}\n",
        "$$\n",
        "In an ideal case, this should be exactly 1. If response is larger than 1, it means we are over predicting, and conversely if the reponse is less than 1, we are under predicting\n",
        "\n",
        "To evaluate:\n",
        "1. Copy the evaluating code from before\n",
        "2. Calculate the reponse\n",
        "3. Plot the response\n",
        "\n",
        "4. BONUS: Look at the reponse per particle type, to see if there are any trends\n",
        "\n",
        "\n",
        "If it is unable to train, mess around with the hyperparameters!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bR-b9qk4T8q_"
      },
      "outputs": [],
      "source": [
        "### SOLUTION ###\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lIH4Pc-UBfc"
      },
      "outputs": [],
      "source": [
        "### BONUS ###\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUNnzzv_VR_u"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kv0w7899cVjH"
      },
      "source": [
        "# Conclusion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gZ9xfrknVSaT"
      },
      "source": [
        "Well done for making it to the end of the notebook! See if you can improve the regression!\n",
        "\n",
        "Other things you could look at (if you havent already)\n",
        "\n",
        "1. Normalisation of the data\n",
        "2. Removing some of the data\n",
        "3. Principle Component Analysis\n",
        "4. K-fold Validation\n",
        "5. Classification and Regression at the same time\n",
        "\n",
        "\n",
        "This material was heavily inspired by the [SLAC Summer Institute Jet Notebooks](https://github.com/makagan/SSI_Projects/tree/main/jet_notebooks)\n",
        "\n",
        "For those who want more complicated tutorials, there are some there"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5qKvSoKyVqav"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
